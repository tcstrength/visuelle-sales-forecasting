{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "prefix = os.getcwd().split(\"/src\")[0]\n",
    "src_dir = os.path.join(prefix, \"src\")\n",
    "data_dir = os.path.join(prefix, \"dataset\")\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tccuong1404/Documents/Projects/visuelle-sales-forecasting/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/tccuong1404/Documents/Projects/visuelle-sales-forecasting/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import mlflow\n",
    "import math\n",
    "from torch import nn\n",
    "from models import VisuelleDataset\n",
    "from utils import misc_utils\n",
    "from torch import optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-28 22:48:17.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.visuelle_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m====== DATASET ======\u001b[0m\n",
      "\u001b[32m2024-02-28 22:48:17.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.visuelle_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mTrain path: /Users/tccuong1404/Documents/Projects/visuelle-sales-forecasting/dataset/train.csv\u001b[0m\n",
      "\u001b[32m2024-02-28 22:48:17.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.visuelle_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTest path: /Users/tccuong1404/Documents/Projects/visuelle-sales-forecasting/dataset/test.csv\u001b[0m\n",
      "\u001b[32m2024-02-28 22:48:17.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.visuelle_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mCategory path: /Users/tccuong1404/Documents/Projects/visuelle-sales-forecasting/dataset/category_labels.pt\u001b[0m\n",
      "\u001b[32m2024-02-28 22:48:17.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.visuelle_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mColor path: /Users/tccuong1404/Documents/Projects/visuelle-sales-forecasting/dataset/color_labels.pt\u001b[0m\n",
      "\u001b[32m2024-02-28 22:48:17.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.visuelle_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mFabric path: /Users/tccuong1404/Documents/Projects/visuelle-sales-forecasting/dataset/fabric_labels.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "vis_dataset = VisuelleDataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 4079/4079 [00:04<00:00, 825.72it/s]\n",
      "100%|##########| 497/497 [00:00<00:00, 821.05it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader = vis_dataset.get_data_loader(train=True, batch_size=64)\n",
    "test_loader = vis_dataset.get_data_loader(train=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, attrs, temporals, gtrends, vtrends = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerPoolingForecast(nn.Module):\n",
    "#     def __init__(self, num_trends: int=4):\n",
    "#         super(TransformerPoolingForecast, self).__init__()\n",
    "#         self.encoder = nn.TransformerEncoder(\n",
    "#             nn.TransformerEncoderLayer(d_model=num_trends, nhead=2, dim_feedforward=256), \n",
    "#             num_layers=1\n",
    "#         )\n",
    "#         self.pooling = nn.AdaptiveAvgPool2d((1, 32))\n",
    "#         self.linear = nn.Linear(32, 12)\n",
    "#         self.criterion = nn.MSELoss()\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=0.1)\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         ### Expect x shape (batch_size, num_trends, trend_len)\n",
    "#         x = self.encoder(x.permute(0, 2, 1))\n",
    "#         x = self.pooling(x)\n",
    "#         x = self.linear(x)\n",
    "#         x = x.view(-1, 12)\n",
    "#         loss = self.criterion(y, x)\n",
    "#         return loss\n",
    "    \n",
    "#     def predict(self, x):\n",
    "#         x = self.encoder(x.permute(0, 2, 1))\n",
    "#         x = self.pooling(x)\n",
    "#         x = self.linear(x)\n",
    "#         x = x.view(-1, 12)\n",
    "#         x = x.relu()\n",
    "#         return x\n",
    "    \n",
    "#     def train(self, data_loader, test_loader, wape_call, epoch: int):\n",
    "#         batch_bar = tqdm(data_loader, desc=f\"Epoch {0}, loss={0:.5f}\")\n",
    "#         for i in range(epoch):\n",
    "#             batch_bar.reset()\n",
    "#             sum_loss = 0\n",
    "#             for batch in data_loader:\n",
    "#                 targets, _, _, _, vtrends = batch\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 loss = self.forward(\n",
    "#                     vtrends, targets\n",
    "#                 )\n",
    "#                 loss.backward()\n",
    "#                 sum_loss += loss\n",
    "#                 self.optimizer.step()\n",
    "#                 batch_bar.update()\n",
    "#                 batch_bar.refresh()\n",
    "\n",
    "#             batch_bar.set_description(f\"Epoch {i}, loss={sum_loss:.5f}\")\n",
    "\n",
    "#             targets, _, _, _, vtrends = test_loader.dataset[:]\n",
    "#             outputs = self.predict(vtrends)\n",
    "#             mlflow.log_metric(\"val_wape\", wape_call(targets, outputs), step=i)\n",
    "\n",
    "#             targets, _, _, _, vtrends = data_loader.dataset[:]\n",
    "#             outputs = self.predict(vtrends)\n",
    "#             mlflow.log_metric(\"train_wape\", wape_call(targets, outputs), step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 12])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerPoolingForecast(4)\n",
    "model.predict(vtrends).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train(train_loader, test_loader, misc_utils.cal_wape, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from fairseq.optim.adafactor import Adafactor\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=52):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    # Takes any module and stacks the time dimension with the batch dimenison of inputs before applying the module\n",
    "    # Insipired from https://keras.io/api/layers/recurrent_layers/time_distributed/\n",
    "    # https://discuss.pytorch.org/t/any-pytorch-function-can-work-as-keras-timedistributed/1346/4\n",
    "    def __init__(self, module, batch_first=True):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module # Can be any layer we wish to apply like Linear, Conv etc\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  \n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y\n",
    "\n",
    "class FusionNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, use_img, use_text, dropout=0.2):\n",
    "        super(FusionNetwork, self).__init__()\n",
    "        \n",
    "        self.img_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.img_linear = nn.Linear(2048, embedding_dim)\n",
    "        self.use_img = use_img\n",
    "        self.use_text = use_text\n",
    "        input_dim = embedding_dim + (embedding_dim*use_img) + (embedding_dim*use_text)\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Linear(input_dim, input_dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_encoding, text_encoding, dummy_encoding):\n",
    "        # Fuse static features together\n",
    "\n",
    "        # Build input\n",
    "        decoder_inputs = []\n",
    "        if self.use_img == 1:\n",
    "            pooled_img = self.img_pool(img_encoding)\n",
    "            condensed_img = self.img_linear(pooled_img.flatten(1))\n",
    "            decoder_inputs.append(condensed_img) \n",
    "        if self.use_text == 1:\n",
    "            decoder_inputs.append(text_encoding) \n",
    "        decoder_inputs.append(dummy_encoding)\n",
    "        concat_features = torch.cat(decoder_inputs, dim=1)\n",
    "\n",
    "        final = self.feature_fusion(concat_features)\n",
    "        # final = self.feature_fusion(dummy_encoding)\n",
    "\n",
    "        return final\n",
    "\n",
    "class GTrendEmbedder(nn.Module):\n",
    "    def __init__(self, forecast_horizon, embedding_dim, use_mask, trend_len, num_trends,  gpu_num):\n",
    "        super().__init__()\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.input_linear = TimeDistributed(nn.Linear(num_trends, embedding_dim))\n",
    "        self.pos_embedding = PositionalEncoding(embedding_dim, max_len=trend_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dropout=0.2)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.use_mask = use_mask\n",
    "        self.gpu_num = gpu_num\n",
    "\n",
    "    def _generate_encoder_mask(self, size, forecast_horizon):\n",
    "        mask = torch.zeros((size, size))\n",
    "        split = math.gcd(size, forecast_horizon)\n",
    "        for i in range(0, size, split):\n",
    "            mask[i:i+split, i:i+split] = 1\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
    "        return mask\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, gtrends):\n",
    "        gtrend_emb = self.input_linear(gtrends.permute(0,2,1))\n",
    "        gtrend_emb = self.pos_embedding(gtrend_emb.permute(1,0,2))\n",
    "        input_mask = self._generate_encoder_mask(gtrend_emb.shape[0], self.forecast_horizon)\n",
    "        if self.use_mask == 1:\n",
    "            gtrend_emb = self.encoder(gtrend_emb, input_mask)\n",
    "        else:\n",
    "            gtrend_emb = self.encoder(gtrend_emb)\n",
    "        return gtrend_emb\n",
    "    \n",
    "    \n",
    "class DummyEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.day_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.week_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.month_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.year_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.dummy_fusion = nn.Linear(embedding_dim*4, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, temporal_features):\n",
    "        # Temporal dummy variables (day, week, month, year)\n",
    "        d, w, m, y = temporal_features[:, 0].unsqueeze(1), temporal_features[:, 1].unsqueeze(1), \\\n",
    "            temporal_features[:, 2].unsqueeze(1), temporal_features[:, 3].unsqueeze(1)\n",
    "        d_emb, w_emb, m_emb, y_emb = self.day_embedding(d), self.week_embedding(w), self.month_embedding(m), self.year_embedding(y)\n",
    "        temporal_embeddings = self.dummy_fusion(torch.cat([d_emb, w_emb, m_emb, y_emb], dim=1))\n",
    "        temporal_embeddings = self.dropout(temporal_embeddings)\n",
    "\n",
    "        return temporal_embeddings\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerDecoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask = None, memory_mask = None, tgt_key_padding_mask = None, \n",
    "            memory_key_padding_mask = None):\n",
    "\n",
    "        tgt2, attn_weights = self.multihead_attn(tgt, memory, memory)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTM(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_heads, num_layers, trend_len, num_trends, use_encoder_mask=1, autoregressive=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_len = output_dim\n",
    "        self.use_encoder_mask = use_encoder_mask\n",
    "        self.autoregressive = autoregressive\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "         # Encoder\n",
    "        self.dummy_encoder = DummyEmbedder(embedding_dim)\n",
    "        self.gtrend_encoder = GTrendEmbedder(output_dim, hidden_dim, use_encoder_mask, trend_len, num_trends, 0)\n",
    "        self.static_feature_encoder = FusionNetwork(embedding_dim, hidden_dim, False, False)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_linear = TimeDistributed(nn.Linear(1, hidden_dim))\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=self.hidden_dim, nhead=num_heads, \\\n",
    "                                                dim_feedforward=self.hidden_dim * 4, dropout=0.1)\n",
    "        \n",
    "        if self.autoregressive: self.pos_encoder = PositionalEncoding(hidden_dim, max_len=12)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, self.output_len if not self.autoregressive else 1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.val_outputs = []\n",
    "        self.val_targets = []\n",
    "\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, temporals, trends):\n",
    "        # Encode features and get inputs\n",
    "        # img_encoding = self.image_encoder(images)\n",
    "        dummy_encoding = self.dummy_encoder(temporals)\n",
    "        # text_encoding = self.text_encoder(category, color, fabric)\n",
    "        gtrend_encoding = self.gtrend_encoder(trends)\n",
    "\n",
    "        # Fuse static features together\n",
    "        static_feature_fusion = self.static_feature_encoder(None, None, dummy_encoding)\n",
    "\n",
    "        if self.autoregressive == 1:\n",
    "            # Decode\n",
    "            tgt = torch.zeros(self.output_len, gtrend_encoding.shape[1], gtrend_encoding.shape[-1]).to(device)\n",
    "            tgt[0] = static_feature_fusion\n",
    "            tgt = self.pos_encoder(tgt)\n",
    "            tgt_mask = self._generate_square_subsequent_mask(self.output_len)\n",
    "            memory = gtrend_encoding\n",
    "            decoder_out, attn_weights = self.decoder(tgt, memory, tgt_mask)\n",
    "            forecast = self.decoder_fc(decoder_out)\n",
    "        else:\n",
    "            # Decode (generatively/non-autoregressively)\n",
    "            tgt = static_feature_fusion.unsqueeze(0)\n",
    "            memory = gtrend_encoding\n",
    "            decoder_out, attn_weights = self.decoder(tgt, memory)\n",
    "            forecast = self.decoder_fc(decoder_out)\n",
    "\n",
    "        return forecast.view(-1, self.output_len), attn_weights\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adafactor(self.parameters(),scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "        return [optimizer]\n",
    "\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        # item_sales, category, color, fabric, temporal_features, gtrends, images = train_batch \n",
    "        targets, attrs, temporals, gtrends, vtrends = train_batch\n",
    "        outs, _ = self.forward(temporals, vtrends)\n",
    "        loss = F.mse_loss(targets, outs)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, test_batch, batch_idx):\n",
    "        targets, attrs, temporals, gtrends, vtrends = test_batch\n",
    "        outs, _ = self.forward(temporals, vtrends)\n",
    "        self.val_outputs.extend(outs)\n",
    "        self.val_targets.extend(targets)\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        outs = torch.stack(self.val_outputs)\n",
    "        targets = torch.stack(self.val_targets)\n",
    "\n",
    "        print(\"val_loss\", F.mse_loss(targets, outs))\n",
    "        print(\"val_wape\", misc_utils.cal_wape(targets, outs))\n",
    "        \n",
    "        self.val_outputs.clear()\n",
    "        self.val_targets.clear()\n",
    "\n",
    "    # def validation_epoch_end(self, val_step_outputs):\n",
    "    #     item_sales, forecasted_sales = [x[0] for x in val_step_outputs], [x[1] for x in val_step_outputs]\n",
    "    #     item_sales, forecasted_sales = torch.stack(item_sales), torch.stack(forecasted_sales)\n",
    "    #     rescaled_item_sales, rescaled_forecasted_sales = item_sales*1065, forecasted_sales*1065 # 1065 is the normalization factor (max of the sales of the training set)\n",
    "    #     loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
    "    #     mae = F.l1_loss(rescaled_item_sales, rescaled_forecasted_sales)\n",
    "        \n",
    "\n",
    "    #     print('Validation MAE:', mae.detach().cpu().numpy(), 'LR:', self.optimizers().param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTM(\n",
    "    embedding_dim=32, hidden_dim=64, output_dim=12, num_heads=4, num_layers=1, trend_len=196, num_trends=4,\n",
    "    # autoregressive=True, use_encoder_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                   | Type               | Params\n",
      "--------------------------------------------------------------\n",
      "0 | dummy_encoder          | DummyEmbedder      | 4.4 K \n",
      "1 | gtrend_encoder         | GTrendEmbedder     | 562 K \n",
      "2 | static_feature_encoder | FusionNetwork      | 68.8 K\n",
      "3 | decoder_linear         | TimeDistributed    | 128   \n",
      "4 | decoder                | TransformerDecoder | 50.0 K\n",
      "5 | decoder_fc             | Sequential         | 780   \n",
      "--------------------------------------------------------------\n",
      "686 K     Trainable params\n",
      "0         Non-trainable params\n",
      "686 K     Total params\n",
      "2.747     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  4.21it/s]val_loss tensor(0.0040, device='mps:0')\n",
      "val_wape 64.131\n",
      "Epoch 0: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, v_num=13]         val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.607\n",
      "Epoch 1: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.503\n",
      "Epoch 2: 100%|██████████| 64/64 [00:28<00:00,  2.28it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.552\n",
      "Epoch 3: 100%|██████████| 64/64 [00:28<00:00,  2.26it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.366\n",
      "Epoch 4: 100%|██████████| 64/64 [00:28<00:00,  2.27it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 58.334\n",
      "Epoch 5: 100%|██████████| 64/64 [00:28<00:00,  2.26it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.258\n",
      "Epoch 6: 100%|██████████| 64/64 [00:28<00:00,  2.26it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.541\n",
      "Epoch 7: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.563\n",
      "Epoch 8: 100%|██████████| 64/64 [00:29<00:00,  2.17it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.514\n",
      "Epoch 9: 100%|██████████| 64/64 [00:28<00:00,  2.22it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.478\n",
      "Epoch 10: 100%|██████████| 64/64 [00:28<00:00,  2.27it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 58.83\n",
      "Epoch 11: 100%|██████████| 64/64 [00:28<00:00,  2.26it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.137\n",
      "Epoch 12: 100%|██████████| 64/64 [00:29<00:00,  2.19it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 58.635\n",
      "Epoch 13: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 58.632\n",
      "Epoch 14: 100%|██████████| 64/64 [00:28<00:00,  2.23it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 58.547\n",
      "Epoch 15: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s, v_num=13]val_loss tensor(0.0028, device='mps:0')\n",
      "val_wape 59.039\n",
      "Epoch 16: 100%|██████████| 64/64 [00:28<00:00,  2.24it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 58.87\n",
      "Epoch 17: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.887\n",
      "Epoch 18: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.718\n",
      "Epoch 19: 100%|██████████| 64/64 [00:28<00:00,  2.26it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 58.741\n",
      "Epoch 20: 100%|██████████| 64/64 [00:28<00:00,  2.23it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 58.704\n",
      "Epoch 21: 100%|██████████| 64/64 [00:28<00:00,  2.24it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.662\n",
      "Epoch 22: 100%|██████████| 64/64 [00:28<00:00,  2.22it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 58.375\n",
      "Epoch 23: 100%|██████████| 64/64 [00:27<00:00,  2.29it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.123\n",
      "Epoch 24: 100%|██████████| 64/64 [00:27<00:00,  2.29it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.238\n",
      "Epoch 25: 100%|██████████| 64/64 [00:27<00:00,  2.34it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 58.401\n",
      "Epoch 26: 100%|██████████| 64/64 [00:27<00:00,  2.33it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.52\n",
      "Epoch 27: 100%|██████████| 64/64 [00:27<00:00,  2.33it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.646\n",
      "Epoch 28: 100%|██████████| 64/64 [00:27<00:00,  2.33it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.359\n",
      "Epoch 29: 100%|██████████| 64/64 [00:27<00:00,  2.34it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 58.627\n",
      "Epoch 30: 100%|██████████| 64/64 [00:27<00:00,  2.33it/s, v_num=13]val_loss tensor(0.0025, device='mps:0')\n",
      "val_wape 59.777\n",
      "Epoch 31: 100%|██████████| 64/64 [00:27<00:00,  2.33it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 60.186\n",
      "Epoch 32: 100%|██████████| 64/64 [00:27<00:00,  2.33it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 58.834\n",
      "Epoch 33: 100%|██████████| 64/64 [00:27<00:00,  2.33it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.034\n",
      "Epoch 34: 100%|██████████| 64/64 [00:27<00:00,  2.34it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.265\n",
      "Epoch 35: 100%|██████████| 64/64 [00:27<00:00,  2.33it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.046\n",
      "Epoch 36: 100%|██████████| 64/64 [00:27<00:00,  2.33it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.99\n",
      "Epoch 37: 100%|██████████| 64/64 [00:27<00:00,  2.34it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 59.358\n",
      "Epoch 38: 100%|██████████| 64/64 [00:27<00:00,  2.33it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.084\n",
      "Epoch 39: 100%|██████████| 64/64 [00:27<00:00,  2.32it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.824\n",
      "Epoch 40: 100%|██████████| 64/64 [00:27<00:00,  2.32it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.08\n",
      "Epoch 41: 100%|██████████| 64/64 [00:30<00:00,  2.09it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.488\n",
      "Epoch 42: 100%|██████████| 64/64 [00:29<00:00,  2.18it/s, v_num=13]val_loss tensor(0.0025, device='mps:0')\n",
      "val_wape 60.553\n",
      "Epoch 43: 100%|██████████| 64/64 [00:29<00:00,  2.21it/s, v_num=13]val_loss tensor(0.0025, device='mps:0')\n",
      "val_wape 60.699\n",
      "Epoch 44: 100%|██████████| 64/64 [00:28<00:00,  2.23it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 61.138\n",
      "Epoch 45: 100%|██████████| 64/64 [00:29<00:00,  2.20it/s, v_num=13]val_loss tensor(0.0025, device='mps:0')\n",
      "val_wape 63.733\n",
      "Epoch 46: 100%|██████████| 64/64 [00:28<00:00,  2.23it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.892\n",
      "Epoch 47: 100%|██████████| 64/64 [00:28<00:00,  2.26it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 63.089\n",
      "Epoch 48: 100%|██████████| 64/64 [00:28<00:00,  2.26it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 60.575\n",
      "Epoch 49: 100%|██████████| 64/64 [00:28<00:00,  2.24it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 59.386\n",
      "Epoch 50: 100%|██████████| 64/64 [00:28<00:00,  2.24it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 59.139\n",
      "Epoch 51: 100%|██████████| 64/64 [00:28<00:00,  2.24it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 62.666\n",
      "Epoch 52: 100%|██████████| 64/64 [00:28<00:00,  2.24it/s, v_num=13]val_loss tensor(0.0025, device='mps:0')\n",
      "val_wape 63.754\n",
      "Epoch 53: 100%|██████████| 64/64 [00:28<00:00,  2.22it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 66.292\n",
      "Epoch 54: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 60.996\n",
      "Epoch 55: 100%|██████████| 64/64 [00:28<00:00,  2.24it/s, v_num=13]val_loss tensor(0.0028, device='mps:0')\n",
      "val_wape 71.378\n",
      "Epoch 56: 100%|██████████| 64/64 [00:28<00:00,  2.26it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 61.25\n",
      "Epoch 57: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 65.935\n",
      "Epoch 58: 100%|██████████| 64/64 [00:28<00:00,  2.27it/s, v_num=13]val_loss tensor(0.0025, device='mps:0')\n",
      "val_wape 60.963\n",
      "Epoch 59: 100%|██████████| 64/64 [00:29<00:00,  2.13it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 60.988\n",
      "Epoch 60: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 65.748\n",
      "Epoch 61: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 60.843\n",
      "Epoch 62: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, v_num=13]val_loss tensor(0.0028, device='mps:0')\n",
      "val_wape 71.869\n",
      "Epoch 63: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 61.1\n",
      "Epoch 64: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 63.072\n",
      "Epoch 65: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 61.715\n",
      "Epoch 66: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 65.58\n",
      "Epoch 67: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 65.563\n",
      "Epoch 68: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 62.684\n",
      "Epoch 69: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 66.753\n",
      "Epoch 70: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 63.859\n",
      "Epoch 71: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, v_num=13]val_loss tensor(0.0028, device='mps:0')\n",
      "val_wape 61.396\n",
      "Epoch 72: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, v_num=13]val_loss tensor(0.0028, device='mps:0')\n",
      "val_wape 60.804\n",
      "Epoch 73: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 62.968\n",
      "Epoch 74: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 61.432\n",
      "Epoch 75: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 66.029\n",
      "Epoch 76: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, v_num=13]val_loss tensor(0.0029, device='mps:0')\n",
      "val_wape 69.588\n",
      "Epoch 77: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 63.586\n",
      "Epoch 78: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 62.251\n",
      "Epoch 79: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 66.04\n",
      "Epoch 80: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 63.683\n",
      "Epoch 81: 100%|██████████| 64/64 [00:37<00:00,  1.70it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 66.552\n",
      "Epoch 82: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 65.605\n",
      "Epoch 83: 100%|██████████| 64/64 [00:38<00:00,  1.64it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 62.641\n",
      "Epoch 84: 100%|██████████| 64/64 [00:34<00:00,  1.83it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 63.213\n",
      "Epoch 85: 100%|██████████| 64/64 [00:36<00:00,  1.75it/s, v_num=13]val_loss tensor(0.0026, device='mps:0')\n",
      "val_wape 63.457\n",
      "Epoch 86: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s, v_num=13]val_loss tensor(0.0029, device='mps:0')\n",
      "val_wape 69.046\n",
      "Epoch 87: 100%|██████████| 64/64 [00:29<00:00,  2.16it/s, v_num=13]val_loss tensor(0.0027, device='mps:0')\n",
      "val_wape 63.831\n",
      "Epoch 88: 100%|██████████| 64/64 [00:28<00:00,  2.23it/s, v_num=13]val_loss tensor(0.0028, device='mps:0')\n",
      "val_wape 69.572\n",
      "Epoch 89: 100%|██████████| 64/64 [00:28<00:00,  2.27it/s, v_num=13]val_loss tensor(0.0029, device='mps:0')\n",
      "val_wape 65.207\n",
      "Epoch 90: 100%|██████████| 64/64 [00:38<00:00,  1.67it/s, v_num=13]val_loss tensor(0.0029, device='mps:0')\n",
      "val_wape 66.917\n",
      "Epoch 91:  16%|█▌        | 10/64 [00:04<00:23,  2.32it/s, v_num=13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tccuong1404/Documents/Projects/visuelle-sales-forecasting/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator=device, devices=1, max_epochs=100, check_val_every_n_epoch=1)\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
